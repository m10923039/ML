{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\"\"\"\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "print(tf.__version__)\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import * \n",
    "import time\n",
    "\n",
    "tf.config.set_soft_device_placement(True)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "t=time.time()\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    tf.random.set_seed(0)\n",
    "    a = tf.random.uniform((10000,10000),minval = 0,maxval = 3.0)\n",
    "    c = tf.matmul(a, tf.transpose(a))\n",
    "    d = tf.reduce_sum(c)\n",
    "print('gpu: ', time.time()-t)\n",
    "\n",
    "t=time.time()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    tf.random.set_seed(0)\n",
    "    a = tf.random.uniform((10000,10000),minval = 0,maxval = 3.0)\n",
    "    c = tf.matmul(a, tf.transpose(a))\n",
    "    d = tf.reduce_sum(c)\n",
    "print('cpu: ', time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "##定義ImageDataGenerator\n",
    "train_generator = ImageDataGenerator( featurewise_center=True,\n",
    "                             featurewise_std_normalization=True,\n",
    "                             rotation_range=60,\n",
    "                             width_shift_range=0.3,\n",
    "                            height_shift_range=0.3,\n",
    "                             shear_range=0.3,\n",
    "                             zoom_range=0.3,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=True,dtype=np.float32)\n",
    "\n",
    "train_generator = train_generator.flow_from_directory(\n",
    "        r'C:\\Users\\LAB543\\Desktop\\DATA\\opencv_test\\Train_Image\\C',\n",
    "        target_size=(512, 512),    #將圖片大小轉化為512x512\n",
    "        batch_size=31,              #每一次處理32張圖片\n",
    "        class_mode='categorical',  #對類別進行獨熱編碼\n",
    "        save_to_dir=r'C:\\Users\\LAB543\\Desktop\\DATA\\opencv_test\\train_augmentation\\C', #將augmentation之後的圖片保存\n",
    "        save_format='JPG',\n",
    "        save_prefix=\"augmentation_\")\n",
    "\n",
    "count=1\n",
    "for x_batch,y_batch in train_generator:\n",
    "    print(F\"------------開始第{count}次反覆運算-----------------------------\")\n",
    "    print(F\"------------x_batch、y_batch的形狀如下----------------------\")\n",
    "    print(np.shape(x_batch),np.shape(y_batch))\n",
    "#     print('-------------y_batch列印結果如下-----------------------------')\n",
    "#     print(y_batch)\n",
    "    print('============================================================')\n",
    "    count+=1\n",
    "    if count>200:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下為檔案搬移\n",
    "- 將資料夾格式改為\n",
    "Train_Image  Test_Image \n",
    "\n",
    "```\n",
    "Train_Image/\n",
    "...A/\n",
    "......07999.jpg\n",
    "......07998.jpg\n",
    "...B/\n",
    "......XXXXX.jpg\n",
    "......XXXXX.jpg\n",
    "...C/\n",
    "......CCCCC.jpg\n",
    "......CCCCC.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \t\t\t\t#os是用來切換路徑和創建資料夾的。\n",
    "from shutil import copy #shutil 是用來複製黏貼檔的\n",
    "\n",
    "train_dir = r'C:\\Users\\lab543\\Desktop\\DATA\\Train_Image'\n",
    "train_label = pd.read_csv(train_dir+ r'\\..\\train.csv',header = 0)\n",
    "train_label_list=train_label[train_label['label']=='A'].iloc[:,0].values.tolist()\n",
    "    \n",
    "    \n",
    "file_path = r'C:\\Users\\lab543\\Desktop\\DATA\\Train_Image' #想拆分的資料夾所在路徑,也就是一大堆檔所在的路徑\n",
    "save_dir = r'C:\\Users\\LAB543\\Desktop\\DATA\\opencv_test\\Train_Image' #save_dir 是想把複製出來的檔存放在的路徑\n",
    "dir_name = \"C\" # A B C\n",
    " \n",
    "# 獲取 file_path 下的文件和資料夾清單\n",
    "# 因為 file_path 裡面沒有資料夾，所以不處理有資料夾的情況\n",
    "pathDir = train_label[train_label['label']==dir_name].iloc[:,0].values.tolist() #os.listdir(file_path) 是獲取指定路徑下包含的檔或資料夾清單\n",
    "\n",
    "for filename in pathDir: #遍歷pathDir下的所有檔filename\n",
    "    print(filename)\n",
    "    from_path = os.path.join(file_path, filename) #舊檔的絕對路徑(包含檔的尾碼名)\n",
    "    to_path = save_dir + \"\\\\\" + dir_name          #新檔的絕對路徑\n",
    "    if not os.path.isdir(to_path):# 如果 to_path 目錄不存在，則創建\n",
    "        os.makedirs(to_path)\n",
    "    copy(from_path, to_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAR train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ho\n",
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,models,optimizers\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 可以不用此方法 ， 以image_dataset_from_directory代替\n",
    "# img_resize=(224,224)\n",
    "# def ReFileName(dirPath):\n",
    "#     mango=[]\n",
    "#     for file in os.listdir(dirPath):\n",
    "#         if os.path.isfile(os.path.join(dirPath, file)) == True:\n",
    "#            c= os.path.basename(file)\n",
    "#            name = dirPath + '\\\\' + c\n",
    "#            img = cv2.imread(name)\n",
    "#            img=cv2.resize(img,img_resize)\n",
    "#            mango.append(img)\n",
    "#     return mango"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image_dataset_from_directory 讀取 train_dataset、 val_dataset\n",
    "\n",
    "- labels=\"inferred\" （標籤是從目錄結構生成的） \n",
    "- label_mode=\"categorical\" (對應 One Hat 格式)\n",
    "- class_names=None 按照順序 A->0 B->1 C->2\n",
    "- subset= training,validation, 讀取資料集，是屬於 training、validation\n",
    "- seed=0  training、validation 務必一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_dir=r'C:\\Users\\LAB543\\Desktop\\DATA\\opencv_test\\train_augmentation'\n",
    "train_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=0,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")\n",
    "val_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=0,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), tf.cast(y, dtype=tf.int32)))\n",
    "val_dataset=val_dataset.map(lambda x, y: (normalization_layer(x), tf.cast(y, dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 8,300,227\n",
      "Trainable params: 660,867\n",
      "Non-trainable params: 7,639,360\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Ho\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "def create_model():\n",
    "    \n",
    "    net = VGG16(include_top=False, weights='imagenet', input_tensor=None,\n",
    "#           input_shape = (train_data.shape[1],train_data.shape[2],train_data.shape[3])\n",
    "            input_shape = (224,224, 3) \n",
    "          )\n",
    "    l2 = tf.keras.regularizers.l2(0.01)\n",
    "\n",
    "    # 透過layer_name 節取 model的某一層 block3_pool block4_conv3\n",
    "    layer_name='block4_pool'\n",
    "\n",
    "    # 設定取到該層的都使用預設參數，不訓練參數\n",
    "    layer_index=net.layers.index(net.get_layer(layer_name))\n",
    "\n",
    "    # 建立模型\n",
    "    base_model = net.get_layer(layer_name).output\n",
    "    base_model = layers.Conv2D(128, (1, 1), padding='valid')(base_model)\n",
    "    base_model = layers.MaxPooling2D( pool_size = (3, 3) )(base_model)\n",
    "\n",
    "    # 攤平 feature map \n",
    "    base_model = layers.Flatten()(base_model)\n",
    "\n",
    "\n",
    "    base_model = layers.Dropout(0.3)(base_model)\n",
    "    base_model = layers.BatchNormalization()(base_model)\n",
    "    base_model = layers.Dense(units = 256, kernel_initializer = tf.keras.initializers.GlorotNormal()\n",
    "             ,kernel_regularizer= l2 , activation= 'relu')(base_model)\n",
    "\n",
    "\n",
    "    base_model = layers.Dense(units = 256, kernel_initializer = tf.keras.initializers.GlorotNormal()\n",
    "             ,kernel_regularizer = l2 , activation= 'relu')(base_model)\n",
    "\n",
    "    base_model = layers.Dropout(0.3)(base_model)\n",
    "    output_layer = layers.Dense(3, activation='softmax', name='softmax')(base_model)\n",
    "\n",
    "    # 設定凍結與要進行訓練的網路層\n",
    "    net_final = tf.keras.models.Model(inputs=net.input, outputs=output_layer)\n",
    "\n",
    "    for layer in net_final.layers[:layer_index]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return net_final\n",
    "\n",
    "test_model=create_model()\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  compile fit  callbacks 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "56/56 [==============================] - 32s 460ms/step - loss: 7.6045 - precision: 0.5026 - recall: 0.3833 - accuracy: 0.4778 - val_loss: 5.4425 - val_precision: 0.6224 - val_recall: 0.5824 - val_accuracy: 0.6036\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 23s 397ms/step - loss: 4.7224 - precision: 0.7248 - recall: 0.5387 - accuracy: 0.6447 - val_loss: 3.1685 - val_precision: 0.8106 - val_recall: 0.6804 - val_accuracy: 0.7520\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 23s 401ms/step - loss: 2.9181 - precision: 0.7618 - recall: 0.6363 - accuracy: 0.7164 - val_loss: 1.9971 - val_precision: 0.8540 - val_recall: 0.7726 - val_accuracy: 0.8128\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 24s 404ms/step - loss: 1.8848 - precision: 0.8126 - recall: 0.7233 - accuracy: 0.7679 - val_loss: 1.3381 - val_precision: 0.8608 - val_recall: 0.8221 - val_accuracy: 0.8405\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 24s 406ms/step - loss: 1.3097 - precision: 0.8462 - recall: 0.7821 - accuracy: 0.8179 - val_loss: 1.0341 - val_precision: 0.8526 - val_recall: 0.8302 - val_accuracy: 0.8394\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 24s 408ms/step - loss: 0.9631 - precision: 0.8793 - recall: 0.8442 - accuracy: 0.8636 - val_loss: 0.7121 - val_precision: 0.9600 - val_recall: 0.9397 - val_accuracy: 0.9478\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 24s 405ms/step - loss: 0.7905 - precision: 0.9017 - recall: 0.8698 - accuracy: 0.8879 - val_loss: 0.6033 - val_precision: 0.9683 - val_recall: 0.9553 - val_accuracy: 0.9612\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 23s 403ms/step - loss: 0.6953 - precision: 0.9009 - recall: 0.8774 - accuracy: 0.8935 - val_loss: 0.5339 - val_precision: 0.9576 - val_recall: 0.9453 - val_accuracy: 0.9522\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 23s 403ms/step - loss: 0.5949 - precision: 0.9243 - recall: 0.9052 - accuracy: 0.9128 - val_loss: 0.4300 - val_precision: 0.9849 - val_recall: 0.9810 - val_accuracy: 0.9824\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 24s 405ms/step - loss: 0.5448 - precision: 0.9222 - recall: 0.9034 - accuracy: 0.9127 - val_loss: 0.4198 - val_precision: 0.9733 - val_recall: 0.9673 - val_accuracy: 0.9715\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 24s 406ms/step - loss: 0.5128 - precision: 0.9321 - recall: 0.9122 - accuracy: 0.9228 - val_loss: 0.3841 - val_precision: 0.9753 - val_recall: 0.9698 - val_accuracy: 0.9732\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 24s 407ms/step - loss: 0.4697 - precision: 0.9392 - recall: 0.9316 - accuracy: 0.9367 - val_loss: 0.3335 - val_precision: 0.9899 - val_recall: 0.9874 - val_accuracy: 0.9888\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 24s 408ms/step - loss: 0.4460 - precision: 0.9369 - recall: 0.9261 - accuracy: 0.9307 - val_loss: 0.2998 - val_precision: 0.9947 - val_recall: 0.9930 - val_accuracy: 0.9941\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 24s 405ms/step - loss: 0.3738 - precision: 0.9590 - recall: 0.9503 - accuracy: 0.9542 - val_loss: 0.2881 - val_precision: 0.9910 - val_recall: 0.9885 - val_accuracy: 0.9902\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 24s 407ms/step - loss: 0.3774 - precision: 0.9545 - recall: 0.9468 - accuracy: 0.9495 - val_loss: 0.2585 - val_precision: 0.9941 - val_recall: 0.9936 - val_accuracy: 0.9941\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 24s 405ms/step - loss: 0.3065 - precision: 0.9742 - recall: 0.9693 - accuracy: 0.9714 - val_loss: 0.2545 - val_precision: 0.9871 - val_recall: 0.9863 - val_accuracy: 0.9866\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 24s 411ms/step - loss: 0.2985 - precision: 0.9710 - recall: 0.9663 - accuracy: 0.9683 - val_loss: 0.2267 - val_precision: 0.9927 - val_recall: 0.9908 - val_accuracy: 0.9916\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 24s 414ms/step - loss: 0.3084 - precision: 0.9600 - recall: 0.9549 - accuracy: 0.9579 - val_loss: 0.2207 - val_precision: 0.9950 - val_recall: 0.9936 - val_accuracy: 0.9944\n",
      "INFO:tensorflow:Assets written to: C:/Users/lab543/Desktop/DATA\\Mango_VGG16_02\\assets\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 24s 411ms/step - loss: 0.2991 - precision: 0.9635 - recall: 0.9569 - accuracy: 0.9604 - val_loss: 0.2449 - val_precision: 0.9874 - val_recall: 0.9824 - val_accuracy: 0.9846\n",
      "Epoch 20/100\n",
      "14/56 [======>.......................] - ETA: 9s - loss: 0.3131 - precision: 0.9557 - recall: 0.9482 - accuracy: 0.9555"
     ]
    }
   ],
   "source": [
    "# Ho\n",
    "\n",
    "# \n",
    "# Generate batches of tensor image data with real-time data augmentation.\n",
    "\n",
    "train_datagen=tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=15 , \n",
    "                             width_shift_range=0.2 , \n",
    "                             height_shift_range=0.2 ,\n",
    "                             shear_range=0.2 ,\n",
    "                             zoom_range=0.2 , \n",
    "                             data_format='channels_last')\n",
    "\n",
    "callbacks_list=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "          monitor='val_loss',mode='min', patience=18 ,restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='C:/Users/lab543/Desktop/DATA/Mango_VGG16_02',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy',\n",
    "                             patience=3,\n",
    "                             # 3 epochs 內acc沒下降就要調整LR\n",
    "                             verbose=1,\n",
    "                             factor=0.5,\n",
    "                             # LR降為0.5\n",
    "                             min_lr=0.00001\n",
    "                             # 最小 LR 到0.00001就不再下降\n",
    "                             )]\n",
    "\n",
    "precision=tf.keras.metrics.Precision(name='precision')\n",
    "recall=tf.keras.metrics.Recall(name='recall')\n",
    "accuracy=tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "\n",
    "\n",
    "test_model.compile(optimizer='Adam' , loss='categorical_crossentropy' , metrics=[precision,recall,accuracy])\n",
    "history=test_model.fit(train_dataset,\n",
    "                       validation_data=val_dataset,\n",
    "                       epochs=100,\n",
    "                        callbacks=callbacks_list\n",
    "                      )\n",
    "\n",
    "# history=test_model.fit(train_datagen.flow(train_data,train_label,batch_size=128),\n",
    "#                        epochs=30,\n",
    "#                        validation_data=(val_data,val_label),\n",
    "#                        callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   畫 train 圖  、評估測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HO\n",
    "import matplotlib.pyplot as plot  \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "def train_history_graphic( history       # 資料集合\n",
    "              , history_key1  # 資料集合裡面的來源 1 (有 loss, acc, val_loss, val_acc 四種)\n",
    "              , history_key2  # 資料集合裡面的來源 2 (有 loss, acc, val_loss, val_acc 四種)\n",
    "              , y_label       # Y 軸標籤文字\n",
    "                         ) :\n",
    "    # 資料來源 1\n",
    "    plot.plot( history.history[history_key1] )\n",
    "    # 資料來源 2\n",
    "    plot.plot( history.history[history_key2] )\n",
    "    # 標題\n",
    "    plot.title( 'train history' )\n",
    "\n",
    "    # X 軸標籤文字\n",
    "    plot.xlabel( 'epochs' )\n",
    "\n",
    "    # Y 軸標籤文字\n",
    "    plot.ylabel( y_label )\n",
    "\n",
    "    # 設定圖例\n",
    "    # (參數 1 為圖例說明, 有幾個資料來源, 就對應幾個圖例說明)\n",
    "    # (參數 2 為圖例位置, upper 為上面, lower 為下面, left 為左邊, right 為右邊)\n",
    "    plot.legend( ['train', 'validate']\n",
    "               , loc = 'upper left'\n",
    "               )\n",
    "\n",
    "    # 顯示畫布\n",
    "    plot.show()\n",
    "\n",
    "train_history_graphic( history, 'loss', 'val_loss', 'loss' )\n",
    "train_history_graphic( history, 'accuracy', 'val_accuracy', 'accuracy' )\n",
    "train_history_graphic( history, 'precision', 'val_precision', 'precision' )\n",
    "train_history_graphic( history, 'recall', 'val_recall', 'recall' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示 Model 評估測試資料的\n",
    "\n",
    "test_dir=r'D:\\_YUN\\24_ML\\Work02\\Mango\\DATA\\Test_Image_A_B_C'\n",
    "test_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    interpolation=\"bilinear\"\n",
    ")\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), tf.cast(y, dtype=tf.int32)))\n",
    "\n",
    "# 顯示 Model 評估測試資料的\n",
    "preds=test_model.evaluate(test_dataset)\n",
    "\n",
    "print('loss ： %.5f' % preds[0])\n",
    "print('precision：%.5f' % preds[1])\n",
    "print('recall：%.5f' % preds[2])\n",
    "print('accuracy： %.5f' % preds[3])\n",
    "F1 = 2 * (preds[3] * preds[2]) / (preds[3] + preds[2])\n",
    "print('f1score：%.5f' % F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load_model 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020-05-01 D:\\_YUN\\24_ML\\Work02\\Mango\\ModelDATA\\Mango_VGG16_03\n",
    "# 2021-04-23 load_model\n",
    "# \n",
    "test_model = keras.models.load_model(r'D:\\_YUN\\24_ML\\Work02\\Mango\\ModelDATA\\Mango_VGG16_03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 250 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dir=r'D:\\_YUN\\24_ML\\Work02\\Mango\\DATA\\Test_Image_A_B_C'\n",
    "test_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=True,\n",
    "    interpolation=\"bilinear\"\n",
    ")\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), tf.cast(y, dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 41s 9s/step - loss: 0.7813 - precision: 0.7489 - recall: 0.7040 - accuracy: 0.7360\n",
      "loss ： 0.78128\n",
      "precision：0.74894\n",
      "recall：0.70400\n",
      "accuracy： 0.73600\n",
      "f1score：0.71964\n"
     ]
    }
   ],
   "source": [
    "# 顯示 Model 評估測試資料的\n",
    "preds=test_model.evaluate(test_dataset)\n",
    "\n",
    "print('loss ： %.5f' % preds[0])\n",
    "print('precision：%.5f' % preds[1])\n",
    "print('recall：%.5f' % preds[2])\n",
    "print('accuracy： %.5f' % preds[3])\n",
    "F1 = 2 * (preds[3] * preds[2]) / (preds[3] + preds[2])\n",
    "print('f1score：%.5f' % F1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "0318Churn",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
